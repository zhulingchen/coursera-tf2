{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE for the CelebA dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "In this programming assignment, you will implement the variational autoencoder algorithm for an image dataset of celebrity faces. You will use the trained encoder and decoder networks to reconstruct and generate images. You will also see how the latent space encodes high-level information about the images.\n",
    "\n",
    "Some code cells are provided for you in the notebook. You should avoid editing provided code, and make sure to execute the cells in order to avoid unexpected errors. Some cells begin with the line: \n",
    "\n",
    "`#### GRADED CELL ####`\n",
    "\n",
    "Don't move or edit this first line - this is what the automatic grader looks for to recognise graded cells. These cells require you to write your own code to complete them, and are automatically graded when you submit the notebook. Don't edit the function name or signature provided in these cells, otherwise the automatic grader might not function properly.\n",
    "\n",
    "### How to submit\n",
    "\n",
    "Complete all the tasks you are asked for in the worksheet. When you have finished and are happy with your code, press the Submit Assignment button at the top of this notebook.\n",
    "\n",
    "### Let's get started!\n",
    "\n",
    "We'll start running some imports, and loading the dataset. Do not edit the existing imports in the following cell. If you would like to make further Tensorflow imports, you should add them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar chvfz notebook.tar.gz *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### PACKAGE IMPORTS ####\n",
    "\n",
    "# Run this cell first to import all required packages. Do not make any imports elsewhere in the notebook\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from tensorflow.keras import Sequential, Model\n",
    "from tensorflow.keras.layers import (Dense, Flatten, Reshape, Concatenate, Conv2D, \n",
    "                                     UpSampling2D, BatchNormalization)\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors\n",
    "tfpl = tfp.layers\n",
    "\n",
    "# If you would like to make further imports from tensorflow, add them here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CelebA overview image](data/celeba.png)\n",
    "\n",
    "#### The Large-scale CelebFaces Attributes (CelebA) Dataset\n",
    "\n",
    "For this assignment you will use a subset of the CelebFaces Attributes (CelebA) dataset. The full dataset contains over 200K images CelebA contains thousands of colour images of the faces of celebrities, together with tagged attributes such as 'Smiling', 'Wearing glasses', or 'Wearing lipstick'. It also contains information about bounding boxes and facial part localisation. CelebA is a popular dataset that is commonly used for face attribute recognition, face detection, landmark (or facial part) localization, and face editing & synthesis. \n",
    "\n",
    "* Z. Liu, P. Luo, X. Wang, and X. Tang. \"Deep Learning Face Attributes in the Wild\", Proceedings of International Conference on Computer Vision (ICCV), 2015.\n",
    "\n",
    "You can read about the dataset in more detail [here](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html). \n",
    "\n",
    "Your goal is to implement the variational autoencoder algorithm for a subset of the CelebA dataset. For practical reasons we will keep the dataset and the network size relatively small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the dataset\n",
    "\n",
    "The following functions will be useful for loading and preprocessing the dataset. The subset you will use for this assignment consists of 10,000 training images, 1000 validation images and 1000 test images. These examples have been chosen to respect the original training/validation/test split of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for loading the images\n",
    "\n",
    "def load_dataset(split):\n",
    "    train_list_ds = tf.data.Dataset.from_tensor_slices(np.load('./data/{}.npy'.format(split)))\n",
    "    train_ds = train_list_ds.map(lambda x: (x, x))\n",
    "    return train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training, validation and testing datasets splits\n",
    "\n",
    "train_ds = load_dataset('train')\n",
    "val_ds = load_dataset('val')\n",
    "test_ds = load_dataset('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a few examples\n",
    "\n",
    "n_examples_shown = 6\n",
    "f, axs = plt.subplots(1, n_examples_shown, figsize=(16, 3))\n",
    "\n",
    "for j, image in enumerate(train_ds.take(n_examples_shown)):\n",
    "    axs[j].imshow(image[0])\n",
    "    axs[j].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch the Dataset objects\n",
    "\n",
    "batch_size = 32\n",
    "train_ds = train_ds.batch(batch_size)\n",
    "val_ds = val_ds.batch(batch_size)\n",
    "test_ds = test_ds.batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mixture of Gaussians distribution\n",
    "\n",
    "We will define a prior distribution that is a mixture of Gaussians. This is a more flexible distribution that is comprised of $K$ separate Gaussians, that are combined together with some weighting assigned to each. \n",
    "\n",
    "Recall that the probability density function for a multivariate Gaussian distribution with mean $\\mu\\in\\mathbb{R}^D$ and covariance matrix $\\Sigma\\in\\mathbb{R}^{D\\times D}$ is given by\n",
    "\n",
    "$$\n",
    "\\mathcal{N}(\\mathbf{z}; \\mathbf{\\mu}, \\Sigma) = \\frac{1}{(2\\pi)^{D/2}|\\Sigma|^{1/2}}\n",
    "\\exp\\left(-\\frac{1}{2}(\\mathbf{z}-\\mathbf{\\mu})^T\\Sigma^{-1}(\\mathbf{z}-\\mathbf{\\mu})\\right).\n",
    "$$\n",
    "\n",
    "A mixture of Gaussians with $K$ components defines $K$ Gaussians defined by means $\\mathbf{\\mu}_k$ and covariance matrices $\\Sigma_k$, for $k=1,\\ldots,K$. It also requires mixing coefficients $\\pi_k$, $k=1,\\ldots,K$ with $\\sum_{k} \\pi_k = 1$. These coefficients define a categorical distribution over the $K$ Gaussian components. To sample an event, we first sample from the categorical distribution, and then again from the corresponding Gaussian component.\n",
    "\n",
    "The probability density function of the mixture of Gaussians is simply the weighted sum of probability density functions for each Gaussian component:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{z}) = \\sum_{k=1}^K \\pi_k \\mathcal{N}(\\mathbf{z}; \\mathbf{\\mu}_k, \\Sigma_k)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the prior distribution\n",
    "\n",
    "You should now complete the following function to define the mixture of Gaussians distribution for the prior, for a given number of components and latent space dimension. Each Gaussian component will have a diagonal covariance matrix. This distribution will have fixed mixing coefficients, but trainable means and standard deviations. \n",
    "\n",
    "* The function takes `num_modes` (number of components) and `latent_dim` as arguments\n",
    "* Use the `tfd.MixtureSameFamily` for the prior distribution. Take a look at [the documentation](https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/MixtureSameFamily) for this distribution\n",
    "  * The constructor takes a `mixture_distribution` and `components_distribution` as required arguments\n",
    "  * The `mixture_distribution` should be fixed to a uniform `tfd.Categorical` distribution, so that $pi_k = 1/K$ in the above equation. This argument will therefore not contain any trainable variables\n",
    "  * The `components_distribution` should be a `tfd.MultivariateNormalDiag` distribution batch shape equal to `[num_modes]` and event shape equal to `[latent_dim]`. \n",
    "    * The `tfd.MultivariateNormalDiag` distribution should have trainable `loc` parameter (initialised with a random normal distribution)  and trainable `scale_diag` parameter (initialised to ones)\n",
    "    * The `scale_diag` variable should be enforced to be positive using `tfp.util.TransformedVariable` and the `tfb.Softplus` bijection\n",
    "\n",
    "The function should return the instance of the `tfd.MixtureSameFamily` distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function. \n",
    "# Make sure to not change the function name or arguments.\n",
    "\n",
    "def get_prior(num_modes, latent_dim):\n",
    "    \"\"\"\n",
    "    This function should create an instance of a MixtureSameFamily distribution \n",
    "    according to the above specification. \n",
    "    The function takes the num_modes and latent_dim as arguments, which should \n",
    "    be used to define the distribution.\n",
    "    Your function should then return the distribution instance.\n",
    "    \"\"\"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run your function to get the prior distribution with 2 components and latent_dim = 50\n",
    "\n",
    "prior = get_prior(num_modes=2, latent_dim=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the encoder network\n",
    "\n",
    "We will now define the encoder network as part of the VAE. First, we will define the `KLDivergenceRegularizer` to use in the encoder network to add the KL divergence part of the loss. This should be defined according to the following specification:\n",
    "\n",
    "* The function takes the `prior_distribution` as an argument\n",
    "* The function should use the `tfpl.KLDivergenceRegularizer` object to add the KL loss term\n",
    "* The `tfpl.KLDivergenceRegularizer` should use a weight factor of 1.0 for the KL loss (standard ELBO objective)\n",
    "* The KL loss cannot be computed exactly, so the `tfpl.KLDivergenceRegularizer` should compute a Monte Carlo approximation by drawing 3 samples from the posterior, and then averaging over the sample and batch axes\n",
    "\n",
    "Your function should then return the `tfpl.KLDivergenceRegularizer` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function. \n",
    "# Make sure to not change the function name or arguments.\n",
    "\n",
    "def get_kl_regularizer(prior_distribution):\n",
    "    \"\"\"\n",
    "    This function should create an instance of the KLDivergenceRegularizer \n",
    "    according to the above specification. \n",
    "    The function takes the prior_distribution, which should be used to define \n",
    "    the distribution.\n",
    "    Your function should then return the KLDivergenceRegularizer instance.\n",
    "    \"\"\"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run your function to get the KLDivergenceRegularizer\n",
    "\n",
    "kl_regularizer = get_kl_regularizer(prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now complete the following function to define the encoder network, according to the following specification:\n",
    "\n",
    "* The function takes the `latent_dim` and `kl_regularizer` as arguments\n",
    "* Use the `Sequential` class to define the model\n",
    "  * The first layer should be a Conv2D layer with 32 filters, 4x4 kernel size, ReLU activation, stride of 2x2, and 'SAME' padding. It should also set the `input_shape` to `(64, 64, 3)`\n",
    "  * The second layer should be a BatchNormalization layer\n",
    "  * The third layer should be a Conv2D layer with 64 filters, 4x4 kernel size, ReLU activation, stride of 2x2, and 'SAME' padding\n",
    "  * The fourth layer should be a BatchNormalization layer\n",
    "  * The fifth layer should be a Conv2D layer with 128 filters, 4x4 kernel size, ReLU activation, stride of 2x2, and 'SAME' padding\n",
    "  * The sixth layer should be a BatchNormalization layer\n",
    "  * The seventh layer should be a Conv2D layer with 256 filters, 4x4 kernel size, ReLU activation, stride of 2x2, and 'SAME' padding\n",
    "  * The eighth layer should be a BatchNormalization layer\n",
    "  * The ninth layer should be a Flatten layer\n",
    "  * The tenth layer should be a Dense layer with no activation function, and the right number of units to parameterise a `MultivariateNormalTriL` layer with event size equal to `latent_dim`\n",
    "  * The final layer should be a `MultivariateNormalTriL` layer with event size equal to `latent_dim`, and it should use the `kl_regularizer` passed in as the argument as the activity regularizer\n",
    "\n",
    "In total, your model should have 11 layers.\n",
    "\n",
    "The function should then return the encoder model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "\n",
    "# Complete the following function. \n",
    "# Make sure to not change the function name or arguments.\n",
    "\n",
    "def get_encoder(latent_dim, kl_regularizer):\n",
    "    \"\"\"\n",
    "    This function should build a CNN encoder model according to the above specification. \n",
    "    The function takes latent_dim and kl_regularizer as arguments, which should be\n",
    "    used to define the model.\n",
    "    Your function should return the encoder model.\n",
    "    \"\"\"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run your function to get the encoder\n",
    "\n",
    "encoder = get_encoder(latent_dim=50, kl_regularizer=kl_regularizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the encoder summary\n",
    "\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the decoder network\n",
    "\n",
    "You should now define the decoder network for the VAE, using the `Sequential` API. This should be a neural network that returns an IndependentBernoulli distribution of `event_shape=(64, 64, 3)`.\n",
    "\n",
    "* The function takes the `latent_dim` as an argument\n",
    "* Use the `Sequential` class to define the model\n",
    "  * The first layer should be a Dense layer with 4096 units and ReLU activation. It should also set the `input_shape` to `(latent_dim,)`\n",
    "  * The second layer should be a Reshape layer, that reshapes its input to `(4, 4, 256)`\n",
    "  * The third layer should be an UpSampling2D layer with upsampling factor of `(2, 2)`\n",
    "  * The fourth layer should be a Conv2D layer with 128 filters, 3x3 kernel size, ReLU activation, and 'SAME' padding\n",
    "  * The fifth layer should be an UpSampling2D layer with upsampling factor of `(2, 2)`\n",
    "  * The sixth layer should be a Conv2D layer with 64 filters, 3x3 kernel size, ReLU activation, and 'SAME' padding\n",
    "  * The seventh layer should be an UpSampling2D layer with upsampling factor of `(2, 2)`\n",
    "  * The eighth layer should be a Conv2D layer with 32 filters, 3x3 kernel size, ReLU activation, and 'SAME' padding\n",
    "  * The ninth layer should be an UpSampling2D layer with upsampling factor of `(2, 2)`\n",
    "  * The tenth layer should be a Conv2D layer with 128 filters, 3x3 kernel size, ReLU activation, and 'SAME' padding\n",
    "  * The eleventh layer should be a Conv2D layer with 3 filters, 3x3 kernel size, no activation function, and 'SAME' padding\n",
    "  * The twelfth layer should be a Flatten layer\n",
    "  * The final layer should be a `IndependentBernoulli` layer with event size equal to `(64, 64, 3)`\n",
    "\n",
    "In total, your model should have 13 layers.\n",
    "\n",
    "The function should then return the decoder model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #### GRADED CELL ####\n",
    "\n",
    "# Complete the following function. \n",
    "# Make sure to not change the function name or arguments.\n",
    "\n",
    "def get_decoder(latent_dim):\n",
    "    \"\"\"\n",
    "    This function should build a CNN decoder model according to the above specification. \n",
    "    The function takes latent_dim as an argument, which should be used to define the model.\n",
    "    Your function should return the decoder model.\n",
    "    \"\"\"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run your function to get the decoder\n",
    "\n",
    "decoder = get_decoder(latent_dim=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the decoder summary\n",
    "\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Link the encoder and decoder together\n",
    "\n",
    "The following cell connects `encoder` and `decoder` to form the end-to-end architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect the encoder and decoder\n",
    "\n",
    "vae = Model(inputs=encoder.inputs, outputs=decoder(encoder.outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the average reconstruction loss\n",
    "\n",
    "You should now define the reconstruction loss that forms the remaining part of the negative ELBO objective. This function should take a batch of images of shape `(batch_size, 64, 64, 3)` in the first argument, and the output of the decoder after passing the batch of images through `vae` in the second argument. \n",
    "\n",
    "The loss should be defined so that it returns\n",
    "\\begin{equation}\n",
    "    -\\frac{1}{n}\\sum_{i=1}^n \\log p(x_i|z_i)\n",
    "\\end{equation}\n",
    "where $n$ is the batch size and $z_i$ is sampled from $q(z|x_i)$, the encoding distribution a.k.a. the approximate posterior. The value of this expression is always a scalar.\n",
    "\n",
    "Expression (1) is, as you know, is an estimate of the (negative of the) batch's average expected reconstruction loss:\n",
    "\n",
    "\\begin{equation}\n",
    "    -\\frac{1}{n}\\sum_{i=1}^n \\mathrm{E}_{Z\\sim q(z|x_i)}\\big[\\log p(x_i|Z)\\big]\n",
    "\\end{equation}\n",
    "\n",
    "_Hints_ : \n",
    "- _You may find the `log_prob` method of the decoding distribution helpful._\n",
    "- _The code you add does not need to be more than one line._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #### GRADED CELL ####\n",
    "\n",
    "# Complete the following function. \n",
    "# Make sure to not change the function name or arguments.\n",
    "\n",
    "def reconstruction_loss(batch_of_images, decoding_dist):\n",
    "    \"\"\"\n",
    "    This function should compute and return the average expected reconstruction loss,\n",
    "    as defined above.\n",
    "    The function takes batch_of_images (Tensor containing a batch of input images to\n",
    "    the encoder) and decoding_dist (output distribution of decoder after passing the \n",
    "    image batch through the encoder and decoder) as arguments.\n",
    "    The function should return the scalar average expected reconstruction loss.\n",
    "    \"\"\"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile and fit the model\n",
    "\n",
    "It's now time to compile and train the model. This may take some time, so as an alternative, you can also load a pre-trained model below if you wish. To train your own model, it is recommended to make use of the accelerator hardware available on Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
    "vae.compile(optimizer=optimizer, loss=reconstruction_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EITHER... compile and fit the model\n",
    "\n",
    "vae.fit(train_ds, validation_data=val_ds, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OR... load the pre-trained model\n",
    "\n",
    "# ckpt = tf.train.Checkpoint(model=vae)\n",
    "# ckpt.restore(tf.train.latest_checkpoint('./model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "\n",
    "test_loss = vae.evaluate(test_ds)\n",
    "print(\"Test loss: {}\".format(test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute reconstructions of test images\n",
    "\n",
    "We will now take a look at some image reconstructions from the encoder-decoder architecture.\n",
    "\n",
    "You should complete the following function, that uses `encoder` and `decoder` to reconstruct images from the test dataset. This function takes the encoder, decoder and a Tensor batch of test images as arguments. The function should be completed according to the following specification:\n",
    "\n",
    "* Get the mean of the encoding distributions from passing the batch of images into the encoder\n",
    "* Pass these latent vectors through the decoder to get the output distribution\n",
    "\n",
    "Your function should then return the mean of the output distribution, which will be a Tensor of shape `(batch_size, 64, 64, 3)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #### GRADED CELL ####\n",
    "\n",
    "# Complete the following function. \n",
    "# Make sure to not change the function name or arguments.\n",
    "\n",
    "def reconstruct(encoder, decoder, batch_of_images):\n",
    "    \"\"\"\n",
    "    This function should compute reconstructions of the batch_of_images according\n",
    "    to the above instructions.\n",
    "    The function takes the encoder, decoder and batch_of_images as inputs, which\n",
    "    should be used to compute the reconstructions.\n",
    "    The function should then return the reconstructions Tensor.\n",
    "    \"\"\"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run your function to compute reconstructions of random samples from the test dataset\n",
    "\n",
    "n_reconstructions = 7\n",
    "num_test_files = np.load('./data/test.npy').shape[0]\n",
    "test_ds_for_reconstructions = load_dataset('test')\n",
    "for all_test_images, _ in test_ds_for_reconstructions.batch(num_test_files).take(1):\n",
    "    all_test_images_np = all_test_images.numpy()\n",
    "example_images = all_test_images_np[np.random.choice(num_test_files, n_reconstructions, replace=False)]\n",
    "\n",
    "reconstructions = reconstruct(encoder, decoder, example_images).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the reconstructions\n",
    "\n",
    "f, axs = plt.subplots(2, n_reconstructions, figsize=(16, 6))\n",
    "axs[0, n_reconstructions // 2].set_title(\"Original test images\")\n",
    "axs[1, n_reconstructions // 2].set_title(\"Reconstructed images\")\n",
    "for j in range(n_reconstructions):\n",
    "    axs[0, j].imshow(example_images[j])\n",
    "    axs[1, j].imshow(reconstructions[j])\n",
    "    axs[0, j].axis('off')\n",
    "    axs[1, j].axis('off')\n",
    "    \n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample new images from the generative model\n",
    "\n",
    "Now we will sample from the generative model; that is, first sample latent vectors from the prior, and then decode those latent vectors with the decoder.\n",
    "\n",
    "You should complete the following function to generate new images. This function takes the prior distribution and decoder network as arguments, as well as the number of samples to generate. This function should be completed according to the following:\n",
    "\n",
    "* Sample a batch of `n_samples` images from the prior distribution, to obtain a latent vector Tensor of shape `(n_samples, 50)`\n",
    "* Pass this batch of latent vectors through the decoder, to obtain an Independent Bernoulli distribution with batch shape equal to `[n_samples]` and event shape equal to `[64, 64, 3]`.\n",
    "\n",
    "The function should then return the mean of the Bernoulli distribution, which will be a Tensor of shape `(n_samples, 64, 64, 3)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #### GRADED CELL ####\n",
    "\n",
    "# Complete the following function. \n",
    "# Make sure to not change the function name or arguments.\n",
    "\n",
    "def generate_images(prior, decoder, n_samples):\n",
    "    \"\"\"\n",
    "    This function should compute generate new samples of images from the generative model,\n",
    "    according to the above instructions.\n",
    "    The function takes the prior distribution, decoder and number of samples as inputs, which\n",
    "    should be used to generate the images.\n",
    "    The function should then return the batch of generated images.\n",
    "    \"\"\"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run your function to generate new images\n",
    "\n",
    "n_samples = 10\n",
    "sampled_images = generate_images(prior, decoder, n_samples)\n",
    "\n",
    "f, axs = plt.subplots(1, n_samples, figsize=(16, 6))\n",
    "\n",
    "for j in range(n_samples):\n",
    "    axs[j].imshow(sampled_images[j])\n",
    "    axs[j].axis('off')\n",
    "    \n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify generations with attribute vector\n",
    "\n",
    "This final section is ungraded, but you will hopefully find it to be an interesting extension. We will see how the latent space encodes high-level information about the images, even though it has not been trained with any information apart from the images themselves.\n",
    "\n",
    "As mentioned in the introduction, each image in the CelebA dataset is labelled according to the attributes of the person pictured.\n",
    "\n",
    "Run the cells below to load these labels, along with a subset of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load labels and images as a numpy array\n",
    "\n",
    "def load_labels_and_image_arrays(split):\n",
    "    dataset = load_dataset(split)\n",
    "    num_files = np.load('./data/{}.npy'.format(split)).shape[0]\n",
    "    \n",
    "    for all_images, _ in dataset.batch(num_files).take(1):\n",
    "        all_images_np = all_images.numpy()\n",
    "\n",
    "    labels = pd.read_csv('./data/list_attr_celeba_subset.csv')\n",
    "    labels = labels[labels['image_id'].isin(files)]\n",
    "    return labels, all_images_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load labels in a pandas DataFrame, training_subset is a numpy array\n",
    "\n",
    "train_labels, training_subset = load_labels_and_image_arrays('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the attributes contained in the DataFrame\n",
    "\n",
    "train_labels.columns[2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each image is labelled with a binary indicator (1 is True, -1 is False) according to whether it possesses the attribute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View a sample from the labels data\n",
    "\n",
    "train_labels.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select an attribute\n",
    "\n",
    "attribute = 'Smiling'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the images into those that have the attribute, and those that don't\n",
    "\n",
    "attribute_mask = (train_labels[attribute] == 1)\n",
    "images_with_attribute = training_subset[attribute_mask]\n",
    "\n",
    "not_attribute_mask = (train_labels[attribute] == -1)\n",
    "images_without_attribute = training_subset[not_attribute_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the 'attribute vector'\n",
    "\n",
    "We will now encode each of the images that have the chosen attribute into the latent space by passing them through the encoder. We then average the latent codes obtained for all of these images to obtain a single latent code.\n",
    "\n",
    "We then do the same for the images that do not have the chosen attribute. This gives an average latent code for images with the attribute, and an average latent code for images without the attribute. Intuitively speaking, the difference between these two vectors then gives us the 'direction' in latent space that corresponds to the presence of the attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the images with and without the chosen attribute\n",
    "\n",
    "encoded_images_with_attribute = encoder(images_with_attribute)\n",
    "encoded_images_without_attribute = encoder(images_without_attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average the latent vectors for each batch of encodings\n",
    "\n",
    "mean_encoded_images_with_attribute = tf.reduce_mean(encoded_images_with_attribute.mean(), \n",
    "                                                    axis=0, keepdims=True)\n",
    "mean_encoded_images_without_attribute = tf.reduce_mean(encoded_images_without_attribute.mean(), \n",
    "                                                    axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the attribute vector\n",
    "\n",
    "attribute_vector = mean_encoded_images_with_attribute -  mean_encoded_images_without_attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view this attribute vector by decoding it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the decoded attribute vector\n",
    "\n",
    "decoded_a = decoder(attribute_vector).mean()\n",
    "plt.imshow(decoded_a.numpy().squeeze())\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modify reconstructions using the attribute vector\n",
    "\n",
    "We can now use the attribute vector to add the attribute to an image reconstruction, where that attribute wasn't present before. To do this, we can just add the attribute vector to the latent vector encoding of the image, and then decode the result. We can also adjust the strength of the attribute vector by scaling with a multiplicative parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the attribute vector to a sample of images that don't have the attribute\n",
    "\n",
    "n_examples = 7\n",
    "sampled_inx = np.random.choice(images_without_attribute.shape[0], n_examples, replace=False)\n",
    "sample_images_without_attribute = images_without_attribute[sampled_inx]\n",
    "sample_images_encodings = encoder(sample_images_without_attribute)\n",
    "sample_images_reconstructions = decoder(sample_images_encodings).mean()\n",
    "\n",
    "k = 2.5  # Weighting of attribute vector\n",
    "modified_sample_images_encodings = sample_images_encodings + (k * attribute_vector)\n",
    "modified_reconstructions = decoder(modified_sample_images_encodings).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the original images, their reconstructions, and modified reconstructions\n",
    "\n",
    "f, axs = plt.subplots(3, n_examples, figsize=(16, 6))\n",
    "axs[0, n_examples // 2].set_title(\"Original images\")\n",
    "axs[1, n_examples // 2].set_title(\"Reconstructed images\")\n",
    "axs[2, n_examples // 2].set_title(\"Images with added attribute\")\n",
    "for j in range(n_examples):\n",
    "    axs[0, j].imshow(sample_images_without_attribute[j])\n",
    "    axs[1, j].imshow(sample_images_reconstructions[j])\n",
    "    axs[2, j].imshow(modified_reconstructions[j])\n",
    "    for ax in axs[:, j]: ax.axis('off')\n",
    "    \n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could also try removing the attribute from images that possess the attribute, or experiment with a different attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Congratulations for completing this programming assignment! You're now ready to move on to the capstone project for this course."
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "probabilistic-deep-learning-with-tensorflow2",
   "graded_item_id": "QLlai",
   "launcher_item_id": "1G18Y"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
